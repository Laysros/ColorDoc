1
00:00:00,000 --> 00:00:04,200
Let me start out by telling you how the data actually get inserted.

2
00:00:04,200 --> 00:00:06,510
Here no data has been inserted yet,

3
00:00:06,510 --> 00:00:11,260
all we have is the bucket directory and we have space in the bucket directory to

4
00:00:11,260 --> 00:00:16,575
put block addresses in that would point to the places where data records are stored.

5
00:00:16,575 --> 00:00:19,470
The hash function will map that key to an address.

6
00:00:19,470 --> 00:00:22,140
In this case here, it mapped it to the address two.

7
00:00:22,140 --> 00:00:28,000
In address two, sits a block address that block address points to a data block.

8
00:00:28,000 --> 00:00:29,100
On that data block,

9
00:00:29,100 --> 00:00:33,860
we stick that first record which had that key and now we proceed like that.

10
00:00:33,860 --> 00:00:38,850
It might be that additional records will go into this right now or later on

11
00:00:38,850 --> 00:00:44,565
but as we look at how this evolves you're going to get data blocks gradually allocated.

12
00:00:44,565 --> 00:00:47,450
Remember that several records can be stored on each block.

13
00:00:47,450 --> 00:00:52,090
There might not be additional blocks added for every single record we insert.

14
00:00:52,090 --> 00:00:55,110
Also remember, that as we insert the data

15
00:00:55,110 --> 00:01:00,530
an important job of the hash function is to uniformly allocate that data here.

16
00:01:00,530 --> 00:01:05,715
With the example that we have used we got room for about 20 records on each block.

17
00:01:05,715 --> 00:01:09,600
What happened here is that we have inserted data here.

18
00:01:09,600 --> 00:01:12,675
These are all partially full but now

19
00:01:12,675 --> 00:01:16,195
we actually had a second time that this address was hid,

20
00:01:16,195 --> 00:01:18,840
this data block here was full and therefore,

21
00:01:18,840 --> 00:01:22,965
a second data block is allocated to continue to store records

22
00:01:22,965 --> 00:01:27,270
that were mapped in by the hash function to this bucket directory address.

23
00:01:27,270 --> 00:01:28,890
I get more filled in here.

24
00:01:28,890 --> 00:01:32,955
Here's a second one that requires two levels more and more.

25
00:01:32,955 --> 00:01:35,055
It proceeds like this.

26
00:01:35,055 --> 00:01:37,515
Now the hash structure has been built,

27
00:01:37,515 --> 00:01:40,460
the data has been inserted and now we want to use

28
00:01:40,460 --> 00:01:44,000
this hash structure to find a record with a particular key value.

29
00:01:44,000 --> 00:01:45,430
We take the key value,

30
00:01:45,430 --> 00:01:47,770
we give it to the hash function and let's say

31
00:01:47,770 --> 00:01:51,705
the hash function returns this address in the bucket directory.

32
00:01:51,705 --> 00:01:56,060
In that is the block address we need to follow to get to the data block

33
00:01:56,060 --> 00:02:00,940
on which the record with that key value is stored if it's in the database.

34
00:02:00,940 --> 00:02:04,280
We need to access a block of the bucket directory.

35
00:02:04,280 --> 00:02:07,070
We need to access a block of data and

36
00:02:07,070 --> 00:02:10,525
then we'll look at whether the record is there or not.

37
00:02:10,525 --> 00:02:14,570
If the bucket directory page that we need is not in

38
00:02:14,570 --> 00:02:18,720
memory then it's going to cost one block access to get it up to memory.

39
00:02:18,720 --> 00:02:23,445
Then following the Pointer it's going to cost us one block access to get to the data,

40
00:02:23,445 --> 00:02:25,215
that's a grand total of two.

41
00:02:25,215 --> 00:02:27,135
Each one, one to two of them,

42
00:02:27,135 --> 00:02:28,970
each one cost 10 milliseconds.

43
00:02:28,970 --> 00:02:34,850
The cost is going to be about 10 to 20 milliseconds to look-up the data on it.

44
00:02:34,850 --> 00:02:37,140
It could be that a big portion of

45
00:02:37,140 --> 00:02:40,675
the bucket directory would actually fit in memory and stay there.

46
00:02:40,675 --> 00:02:41,840
If that's the case,

47
00:02:41,840 --> 00:02:44,660
then looking this one up would basically,

48
00:02:44,660 --> 00:02:46,555
we used a hash function,

49
00:02:46,555 --> 00:02:50,660
we get the address of that but that we don't pay for because it already in memory.

50
00:02:50,660 --> 00:02:55,450
We follow the pointer so we pay one to pull this block up and we find the record.

51
00:02:55,450 --> 00:02:56,660
That's a single one,

52
00:02:56,660 --> 00:02:58,730
that's only 10 milliseconds.

53
00:02:58,730 --> 00:03:01,045
What is interesting about this is,

54
00:03:01,045 --> 00:03:05,885
all the different look-up times we have seen they dependent on the size of the data,

55
00:03:05,885 --> 00:03:08,310
dependent on the size of the index et cetera.

56
00:03:08,310 --> 00:03:12,880
This one appears to be at least semi constant.

57
00:03:12,880 --> 00:03:16,850
There exist dynamic hash functions that will actually allow this

58
00:03:16,850 --> 00:03:21,725
adress space here to be expanded as to keep the access cost constant.

59
00:03:21,725 --> 00:03:26,000
Of course, if we cannot dynamically expand and we continue

60
00:03:26,000 --> 00:03:28,070
inserting data more and more and more

61
00:03:28,070 --> 00:03:31,000
here than this organization is going to deteriorate.

62
00:03:31,000 --> 00:03:34,570
The more data we insert with the same bucket directory size,

63
00:03:34,570 --> 00:03:38,050
the longer these lists of blocks we'll get.

64
00:03:38,050 --> 00:03:41,585
Notice that the way they were built was such that data is

65
00:03:41,585 --> 00:03:45,335
simply inserted in the order it arrives here when we run out of space.

66
00:03:45,335 --> 00:03:47,415
We go to the next one and the next one.

67
00:03:47,415 --> 00:03:50,945
If change like this become 20, 50,

68
00:03:50,945 --> 00:03:54,290
a 100 blocks long then the search in one of

69
00:03:54,290 --> 00:03:58,550
these linked lists is actually going to be like searching in a heap.

70
00:03:58,550 --> 00:04:02,790
It's very important to make sure that data is uniformly distributed.

71
00:04:02,790 --> 00:04:07,280
That this has the appropriate size to guarantee that this does not get too deep.

72
00:04:07,280 --> 00:04:10,234
In the previous data organization examples,

73
00:04:10,234 --> 00:04:13,265
I've illustrated them by using a phone book example.

74
00:04:13,265 --> 00:04:14,685
Forget about it here.

75
00:04:14,685 --> 00:04:17,800
They exist no such contraption.

