<h1>12. Efficiency </h1></br><h1>1 - Physical Database...</h1> When building database applications, the physical database design and the resulting efficiency is very important. We may have done an outstanding job, designing, and implementing the database itself, and designing and implementing the applications that run on it. However, if they don't run fast enough to be useful, then it all does not matter. So, let's take a look at some of the tools that are available to physically design the database, organize the data, index it, and get to it fast.

<h1>2 - Computer Architecture</h1> To understand some of the many issues involved in physical database design, let us take a look at a simple computer architecture. There are two kinds of storage on memory in a computer: there's the disk where the bulk of data and application programs are stored, then there's main memory, where the programs that are run right now and the data that they are run-on need to be located. The CPU can only run applications on data that's stored in main memory. The BUS is there to allow us to transport data from the disk to main memory and to write it back again if it has been changed. The reason the architecture is like that is the following: Main memory or random access memory is volatile. So, it constantly needs power to keep its state. It is very, very fast. It is small, and it is relatively expensive. On the other hand, secondary memory or the disk as is most often the case, is permanent storage. It does not need power in order to keep it state, it is relatively slow compared to main memory, it is very big compared to main memory, and it is very cheap compared to main memory. It is important that only a tiny fraction of a real database will fit in main memory. So, it is necessary to have for example, proper management programs that will allow us to transport the data from disk to main memory when it's to be operated on and write it back again. People that talk about main memory database are not really talking about databases. They are talking about toys. So, our assumption going forward will be that the vast majority of the database will be stored on disk, and only a tiny fraction will be in main memory at any given point in time.

<h1>3 - Why should you care</h1> Let us briefly discuss why we should care what amount of time it takes to access main memory and to access the disk. So, main memory access time is typically 30 nanoseconds or 0.3 times ten to the power of minus seven seconds. In comparison, disk access time is about 10 milliseconds or 10 to the power of minus two seconds. So, in the amount of time that it would take to do a single disk access, you would be able to do three times 10 to the power of five main memory accesses, that is a very large number. It is actually so large that when we do cost computations in database access, we completely ignore the amount of computing time that spent in main memory and we count only disk access time or what is called the I/O cost.

<h1>4 - Main Memory vs Disk</h1> Using the phone book as a metaphor, I would like to illustrate the difference between main memory access and disk access. Main memory access corresponds pretty well to having the book open on a page, and then process that page. Let us say that would take me a minute to do. The question now is how long time would it take me to open the book to have the next page available for processing? There are five orders of magnitude difference between main memory access and disk access. So, assuming it would take a minute to read this page. It would take 200 days to open the book to the next page. So, I read this in a minute, and then I start opening the book, and opening the book, would be way into next semester 200 days from now, I got the second page open and I read that in a minute. That's the difference.

<h1>5 - Disk</h1> In order to get an understanding of how data is sitting on our disk, let's first take a look at what a disk actually is. So, in this example here, you are seeing the insides of a disk. The disk has a number of plates or platters. For each one of the plates, there is a read/write head that accesses the top of the plate and one that accesses the bottom of the plate. All of these read/write heads are connected together and are operated by an actuator. So, what the actuator will do in order to move the read/write heads further out or further in on the disk is that this one will turn. So, when it turns, then you will see that this will move further out or it will move further in. All the platters move together. They all sit at the same spindle and they all move together. So, with the read/write heads in a particular position and all of these plates spinning together, what passes under the read/write heads is called a cylinder. On each surface, what paces under the read/write head in that position is called a track. So, on the surface here, you have a track and you have a corresponding track on every single surface under the same head position. That collection of tracks is called a cylinder. Each surface is split up into a number of sectors. A sector is the smallest physical unit that could be transported from the disk to main memory. It consists of 512 bytes. Typically though, blocks that will have contributions from several sectors on them is what is transferred as unit. Block typically will be 4K, in other words, it contains eight sectors, or it will be 8K or 16K depending on the type of data we decide to store on the disk. There are several quite interesting videos out on YouTube that shows you disks that are spun open. Some of them show your head crashes and so on. Here's one that's interesting. Go take a look and be impressed when you see how fast the read/write head moves in and out in order to transport data. So, here's an example of a typical higher end disc pack. It would have four plates. Therefore, it will have eight read write heads. It might have approximately 150,000 tracks per surface. A track might be 1,000 kilobytes. Total disk capacity might be 12,000 gigabytes. As I mentioned, a sector is 512 bytes. Disks are formatted, if it's highly formatted data, typical block size would be 4K. If you're dealing with fairly large less formatted objects, you might go higher than that. Most systems by default will come with block size of 4K. Some major systems come with default block size of 8K. A high end disk pack like this would typically have a 600 megabyte per second transfer rate. It would run with 10,000 RPMs and it would have a latency somewhere between three and four milliseconds.

<h1>6 - Records</h1> So now, we know what the disk looks like. Let us take a look first at how records would be stored on a block on a disk. In order to understand that, we need to take a look at what is the size of a record? How do we even determine that? So, let's go back and take a look at one of the relations we've been dealing with in our example namely, the regular use of relation. So, in that relation, we have an email, sex, birthdate, current city, hometown, attribute. We defined previously the email to be a varchar 50. Sex is a single character. Datetime type is eight bytes. Current city and hometown were both varchar 50s. So, if you add these numbers up, 50 plus one plus eight plus 50 and 50, you end up with a record size of 159 bytes. So, here's a block with records allocated from this relation. So, in position one, would be the e-mail of the first record. That would fill out 50 bytes. Then, in 51, we would have the single character that represents sex. Starting in 52, we have birthdate. Then starting in 60, we have the current city. In 110, we have hometown. Then, up at 160, we would be ready to start the email of the second record. So, all in all, each one takes 159 bites.

<h1>7 - Blocks</h1> So, now we have looked at Records. Let's take a look at the blocks they sit on. So, with the example we used it's possible to store several records on a single block. Let's assume that the block size is 4K and let's ignore the metadata that is on it, that will keep information about for example where records start, what their formats are and might contain information sources and next pointer. We assume that we've got 4K space available for data. Let's also assume that we run only about 80 percent full. The reason you might want to not fill a block completely with data is, that if there's some additional record, you need to insert, and there's no room for it then, you would get an overflow and then it takes extra work. So typically, we don't run with blocks totally filled, assuming 80 percent fill. If you've got 80 percent fill and you got 4K block size, then you got about 3,200 bytes with a record size of 159 that means that you will fit about 20 records per block, leaving a little bit of space open. So, here's record number 20. You can see, you might be able to fill some additional records in here. There are two ways of thinking about what happens to the piece of a block that might not fill a complete record at the end if it comes to that. Do we then break up the record we are trying to put in? Put the first part here and the second part on the next block that will be called a spanned representation. If we just ignore the tiny amount of waste at the end, that's called an unspanned representation. In most cases, when multiple records, they will fit on a block. Most database systems will run with unspanned representation, simply to avoid the processing that's necessary to break up records. Obviously, if you have record sizes that are larger than block sizes, you don't have a choice, you have to run with a spanned representation. So, now that we know what a block looks like, let's address what files look like.

<h1>8 - Files</h1> So here's an example of a file. It consists of multiple blocks. These blocks are linked together by address pointers. The blocks may not sit right next to each other on disk, but when we have seen one block, we can use the next pointer to get to the place on disk where the second block is et cetera, et cetera. So those are all the blocks on the file. Continuing with the example, we can assume that block pointers are four bytes in a 32 bit architecture. Let's say, we had four million regular users records and we know that we could fit 20 records on a block. So that means the number of data blocks would be four million divided by 20, which is 200,000 blocks of data. That also means with a block size of 4k, it means that the file size here is about 800 megabytes. Not a big file, not a small one either.

<h1>9 - Assumptions</h1> To be able to compute the amount of time it takes to transport data from disk to main memory and back again, let's just make some assumptions. Typical seek times are between three to eight milliseconds, rotational delays are typically between two and three milliseconds, and transfer times may be somewhere between half a millisecond to one millisecond. If you add these up, you get a total of five to 12 milliseconds so let us conservatively say that a particular page fault is going to cost us 10 milliseconds. While we will make that assumption, we need to make the following observation. Once we are at a particular location on a disk, might there be a chance that instead of just picking up a single block of data, maybe we should pick up 10, 100, 250, because we might need those next in our application? So, bulk transfer or extent transfer would allow, for example, 250 blocks to be transferred in one shot without paying additional seek time and rotational delay. Once we have found the first one of the 250 blocks, we simply just, from that point on, incur transfer time. If we do that with our default of 10 milliseconds per page fault, then this will take us a grand whopping total of 2.5 seconds to transfer the 250 blocks. If, on the other hand, we consider the savings in seek time and rotational delay, then transferring to 250 blocks would only cost us slightly more than a quarter of a second. Very big difference. One of the problems, of course, with extent transfer of bulk transfers is that we are probably going to need more buffer space. When we use the buffer in general and data moves from disk to memory and memory back to disk again, then we need buffer management strategies. In timeshare systems, a favorite strategy used in buffer management is the LRU strategy or the Least Recently Used strategy. The idea is that when we run out of buffer space and we need to free up space for the new data coming in from disk to main memory, then it's what is least recently used that we overwrite. The philosophy being if we haven't used it for the longest time, we're probably not going to use it next. It is not always that an LRU strategy works well from the point of view of a database management system. It would be excellent for merge joins where two relations are sorted on the join fields and you simply merge them together on matching keys. However, if you're doing nested loop joins, then LRU strategies would kill that computation and rather strategies like MRU, or Most Recently Used, might work well for nested loop joins. Much more detail about the join strategies needed in order to make that decision. I just want to give you a heads-up on the importance of buffer management strategies.

<h1>10 - Heap-Unsorted</h1> So, let's take a look at our very first file organization, or rather, in this case, disorganization, because the first one we're going to look at is a heap. A heap is defined as a file of data that is not sorted, so the records sit on a bunch of blocks and there is no sorting among the records. Any record we're looking for in the whole file could sit on any block whatsoever. We saw before that good assumptions, for our example here, would be to have block pointers that are four bytes, four million records with 20 records per block, we need about 200,000 data blocks, and the file size is a total of 800 megs. So, when something is a heap, that basically means that we have to look at the data pages in order to determine whether what we are looking for is going to be in those data pages or not. If there's a grand total of N data pages or N data blocks, then sometimes, we are lucky and the data we are looking for sits on the very first block. Sometimes, we are unlucky and it sits on the very last block. So, on the average, we can expect that we have to look at N over 2 data blocks where N is the total number of data blocks. So, with 200 data blocks, if we are going to look at the average half of those, we'll be looking at 100,000 data blocks. If each one caused the default of 10 milliseconds, that's going to cost us 16.6 minutes to look up a record in a heap.

<h1>11 - Heap setup</h1> So a heap is an unsorted set of data. So let me illustrate to you what it is. So you take the pages of the phonebook, you rip them up, you put them in there, so that's your phonebook right there.

<h1>12 - Heap</h1> Okay, so here's the phone book as a heap. Let's try to see if we can find the phone number of Leomart. Pull a page out of the heap. No, it's not on that page. Pull another page out. No, it's not on that one either. Pull the next one up. No, it's not on that one either. Now sometimes you're lucky when you search in a heap and what you're looking for is on the very first page you pull up. Sometimes you're unlucky and it'll be on the very last page. If you used a phone book a lot and life is long, then on the average, you can be pulling up half the pages from the heap to find what you're looking for. Thank you.

<h1>13 - Sorted but</h1> So, now the data is sorted, as data is in phone books. It should be much much easier to use it to look up a customer. So, let's say, "I'm going to look up the phone number of Leo Mark." All right. So, I open on page number one. I read through it, a, a, a, accurate, affordable. Nope. Leo Mark's not on that page. Look at the second page. Attorney, a basket for you, a best, a better case. No. Third page, discount, upper a, discount, appliance. No. Leo Mark is not on their. No, Leo Marks is not there. No, Leo Mark is not there. Maybe, I should try another approach.

<h1>14 - Sorted Binary</h1> So, data in the phone book is sorted. Let's see how we utilize that in a binary search. We're going to look for the phone number of Leo Mark. So, we take the phone book, we split it magically in half like here, we look at the entry here through the my right, it's New Birth Urban International Ministries. So, Leo Mark comes before them. So, let's split that next piece in half. All right what does it say here? It says friendship. So, it comes after friendship and we are left with this piece here, now let's split that in half, it says here Jesse's flaws. What comes after that? Split that in half and we are here at Lloyd's Insurance. So, it's still in the right side here. We'll split that in half and it says Mack Miller, MC Miller. So, Mack is in the left side. Okay, so here it says magic and here it says, Maine split that in half and see what we end up with here, Mason. All right. We're down pretty small now. Split that in half, let's see which side we are in. All right. That's Mark's. So we're getting close. Would you believe it? Right here on the page are the names that start with Mark. Thank you very much.

<h1>15 - Sorted File</h1> So, it was painful and expensive to lookup data in a heap. So, let's try to sort the data on the file. So now, the records will be sitting here, multiple records on a block, pointed to the next block, several records on that etc, and the data will be sorted here in some order. Now, to look through and find a record with a particular key, what is that going to cost us now? Well, we could start at the first block and look through the records and see if we find the one with the key desired, if not, then we'll look at the second block, see if we find it there, third block, see if we find it there. What is that going to cost us? Well, sometimes the record we are looking for will actually be on the first block. Sometimes the record we are looking for will be on the last block. On the average again, as for unsorted data, we have to look at about half of the blocks of data. Again therefore, the cost of looking up an element or record with a particular key is going to again cost us 16.6 minutes. But the reason for that is that we aren't utilizing the fact that data is sorted that order. So, what if we were to utilize the fact that data is sorted? What if we had a little bit of metadata that would allow us to understand where approximately the middle of the data file is? Then we could go in. We could look at that middle block, and we could determine whether what we are looking for is to the left of where we tried or to the right of where we tried. So, we'll continue with half of the search space. Then in the next step, we'll hit about the middle of that and continue either with the left piece or the right piece. The amount of time it takes us to take something of size and look through it and split in half each time is log base 2 toward this we are searching in. Since we are searching in N blocks, log base 2- N data blocks is what the cost is going to be for binary search in order for us to end up with a single block on which the data element we're looking for is going to be stored if it's in the database. With the numbers we have here, log base 2- N, where N is 200,000, is 18 multiply that by 10 milliseconds, and you get an access time which is 0.18 second. Very, very much faster than the 16.6 minutes. Before we proceed, I want to emphasize that the cost of the search depends on the size of what we search in. So N is the size of the file, and the cost depends on that size.

<h1>16 - Primary Index -Part 1</h1> So in addition to sorting the data and being smart about how we search in it, we might want to consider ways of reducing the size of what we're going to search in. That's where the idea of indices comes in. We are going to look at two kinds of indices; primary indices and secondary indices. So, the idea of a primary index is as follows: we have data here at the bottom and the data is sorted in some order. So, the records come here with increasing or decreasing key values. To build a primary index, what we do is we take a look at the first record and pick up the key of that record and make a copy of it in an index block together with a point of that point's back to the block on which the record with that key was the first one. We go to the second block here, look at the first record, pick up the key value, make a copy, insert it in the index together with a pointer back to the block on which that key value was the key value of the first record. Pick up the key value of the second one, insert it with a pointer etc. When that index block is full we continue. So, the index blocks look like this. There's a key value and a pointer to the record where that was the first key value. So, these look like this. There is a key value followed by a pointer to the block where that key value was the first one, then there's the next key value followed by a pointer, etc. So, that's the content of each one of these. When we come to the very last block of data here and we have picked up the key value of the first record and inserted that, we are done building the index. Notice an interesting detail, since we're picking up the key values in sorted order, once we build the index does not even have reason to sort that, it's born sorted. Also, notice that the only element we're picking up from each record actually is the key value, so all the other current city, birthdate, hometown, etc., all of that is not picked up from here. What does that mean? That means that we can fit a whole lot of entries in an index page. As a matter of fact, let's take a look at exactly how many we could fit in there. So, we know that we have 200,000 blocks of data at the bottom level here. We know that a block pointer is four bytes. We know we only want to fill also the index pages, 80 percent, so we've got 3,200 bytes to work with. We know that the key value, email is 50 characters, varchar 50, you add the 50 to the four byte pointer size, you get 54. Fifty four divided up into 3,200 is going to give you approximately 60. So, when you look at one of these, you're going to have approximately 60 key values and a corresponding set of pointers that come out from that. That's called the fanout. So, with space being utilized 80 percent, the fanout of this primary index is 60. In other words, you're going to see 60 pointers sticking out of each one of these pointing down to the data lab.

<h1>17 - Primary Index -Part 2</h1> What does that mean in terms of how many index blocks we need? Well, we need to point to 200,000, we can point to 60 from each one. So, you divide the 200,000 by 60 and you'll find out that the number of index blocks on this sparse index is 3,334. It would actually be possible to build a primary index as a dense index also. So, instead of picking up the k value of the first block, we pick up every single k value, and then every single one of them will result in an insertion of that k value and a pointer to block one, every single one. Then when we come to the k values here, they are going to be entered together with pointers to block two. It will still be sorted, it will be dense, meaning that all the k values are in the index. The index will certainly be bigger. Using the same numbers, you will actually see that it's about 20 times bigger at 66,000. One advantage of having every single k value is some queries can now actually be s, that do not need to access the data but instead can just access the index. So, it says, for example, what's the maximum k value, the minimum k value, average k value, et cetera, that can be termed by query on the index alone. Also, want to emphasize that primary indexes are great not just to find a particular record with a particular value but they are great at allowing you to do range queries. So, once you find the first one in a range, then of course, you can just follow the next values at the data level or at the index level. So, let's take a look at some lookup times. So, since what we have now is a sorted file, but the file is smaller than the data file here, let us use n to indicate the number of index blocks where previously we used uppercase N to indicate the number of data blocks. So, now the cost of doing a search in this sorted file is therefore log base 2 to lowercase n. But then, once you find the correct value, then of course, you have to go access the data, so we got to add one. So, for the sparse index where the number of blocks is 3,334, to look things up, you basically do log base 2 to 3,334 plus 1 and multiply that by the 10 millisecond for a grand total of 0.13 seconds to do the lookup through this index. Notice that the 0.13 second is lower than the 0.18 seconds we used when we did binary search on the data file. So, it is cheaper because we cut down the size. There's also a penalty involved in that because there's an extra level to access now. But all in all, we went down from 0.18 seconds to 0.13 seconds. The dense index is obviously larger so we can expect the cost to be higher, so log base 2 to the 66,666 plus the 1 is about 17 multiplied by the 10 milliseconds gives you 0.17 seconds. So, even though this index has many more k values, it is actually still cheaper to use the dense index than it was to do the search on the data [inaudible]. Notice that what we are talking about here is just lookup cost. So, in all of these cases, if we are doing some updates, then the cost would be both the lookup cost in order to find what we are updating and then the cost to write things back to disk after they've been changed. In typical simple situations, you would add an extra disk access to write things back. In some cases, if they had changed that affect the index, you will actually have additional cost related to that.

<h1>18 - Sparse Index</h1> To use the phone book to illustrate for you how a single level index might work, we're going to have to make a few changes to that phone book. You all know that at the top of the phone book is the first key that's mentioned on that page listed together with the page number. Let me show you. So for example, on this page here at the top it says Adairsville, page 25. What I'm going to do for you is I'm going to build a single level index. So, here we go starting to build that index. So, I've now started building an index for the phone book. I cut off the key values together with the page number at the top of each page and I started putting them in, in the same order to build my index. So, here's where I am now. So I build the piece of the index here, you see their are key values listed, and together with each key value there's the page number for the page on which that key is the first entry. Now, let's take a look at how we use it to search in the phone book. So we were looking at Adairsville, Adairsville when I look up in the index appears right here, it says page 25, I open up to page 25 and right there indeed I do see Adairsville is the first key value that appears on that page. I can now look through that page for anything that sits between Adairsville and the next entry in the index.

<h1>19 - Secondary Index</h1> Let us now look at secondary indexes. So again here, you have a file with data blocks. These data blocks are sorted on some key value. But what you want now is you want to build an index not on the key value but on some other value. Now, since the other value you want to build an index on will not be in sync with the order of the key values, what will be all over the place building this index is going to be somewhat more painful. Also notice, that since the values here that we are going to build the index on are not sorted, we cannot build a sparse index. So, all indexes that are secondary indexes have to be dense indexes. So, here's the idea looking at the data. We go down to the first record, we find the value of the field we are building the secondary index on. That value we insert into the index here together with the point back to where it came from. Look at the second record, pick up the key, put the pointer in, third recorded and several. So, every single one in our example, 20 records here will have a key value and a corresponding pointer back to this. When we are done with that, we pick up all the key values here with pointers back, pick up all the key values here, and point back et cetera. Continue to add index blocks until we are all done. If you look at what one of these blocks will look like immediately after we have done that, what you're going to see is of course, that the pointers come nicely and point in order here because the way we explain the construction took these in order. So, all the pointers are going to sit like this. But as I mentioned before, the key values are going to not be ordered. So, what we're going to need to do after this first initial step of building the index is we're going to have to sort it. So after the sort, the key values we have picked up for the secondary index are all going to come in order K1, K2, et cetera. But now, the pointers are going to point all over the place to different parts of the file. The implication of that is, that a secondary index is good for point queries only. You cannot trust that once you follow the point or after a particular key value, that the subsequent key values will come after that. As a matter of fact, this is a counter example to that. The course to look up in the secondary index is still a log-based two to the size of it. The only problem is that, you can't utilize that as a starting point for range query. So log-base two to n is as we did before for a dense index. Comes out that the point look up is going to be 0.17 seconds. Notice that I've implicitly made the following, simplifying assumption. I made the assumption that the non-key field we are indexing on here actually had unique values. So, what if a value appears multiple times in that field that we want to build the secondary index on, then we need to consider whether we are grouping those key values together, so they don't have to be repeated all the time, and then keep a bucket of pointers to all the different copies of that key value accessed.

<h1>20 - Multi-Level Index</h1> So, we've seen a couple of good ideas now. We have seen the idea that sorting things and being smart about how we search and sort of things is really helpful. We have seen that reducing the size of the search space by building indices, and then pay a slight extra penalty for accessing the data lever that that is a smart way to proceed. But if it's a good idea once, maybe it's a good idea twice. So, if you look at this index here, that's a sorted file. So, why don't we build an index on the index and an index on the index of the index, etc, until we end up at the root with just a single index page. So, searching through this index would now work as follows: we look at the top level block, we know that block has have fanout of up to 60. So, among those 60 different key values, we pick the place where the key we are searching for either is equal to one of them or fits between two of them, and we follow the appropriate pointer to the next level. At that next level we have one data block, it has a fanout of 60, we fit in the key value where it fits, and follow the pointer to the index plug at the next level. Eventually, we come down to the bottom of the index level and we need to add one in order to get to the data level. So, in this case here, the search time is locked but the base is not too, the base is the fanout of the index. That fanout is substantially higher than the two we used for binary search in a data file. The larger the base of the lock function is the flatter is the log function. So, we can expect the cost here to be smaller, and smaller, and smaller the higher the fanout is. Using the exact numbers of our example here, we actually see that log base 60 to 3334 is actually just two, we add the one to it, that's three axes at 10 milliseconds each. That's 0.03 seconds to access a data block through this multilevel index. The more complicated construct like this becomes, the more levels of indices we put on top, the more sensitive we are to overflow. So, if we for example, are trying to insert data at the bottom level, and although we started out with some slot space at the end, once we come to the point where blog actually needs to be broken up, then the question is, is that going to have any impact on the first level index? If it does, how far is the ripple effect going to be? It would be important to keep the ripple effect smaller, and that's exactly why we have some slot space built in at the end.

<h1>21 - Multi-Level Headshot</h1> So, in a binary search the idea was to cut the search space in two every single time. So, I cut it in two here so it's in half of the space, cut that in two, search in the half the space. The idea of a multilevel index is that the multilevel index will allow us to cut the search space into much smaller pieces in each step. So, using the multilevel index, I can illustrate as follows. You basically take the book, but instead of cutting in two you cut magically, you cut in much smaller pieces like this. Then, one of the pieces is going to qualify for continued search. Let's say that that piece is this piece. So see, we don't have half the book left, we only have a tiny fraction left. So, that piece needs to be cut down again. We used the index at the next level and magically, we cut it into smaller pieces. Drop all of them except for one and we continue the search with that one. So, continuing to search for that one, again, we cut it in smaller pieces, and eventually, we end up with a piece that may just be a single page. The key value we are looking for is on that page, if it's in the database at all. So, that illustrates what a multilevel index does for us.

<h1>22 - Multi-Level Index B-Tree</h1> The most popular of the multi-level indices is one called a B plus Tree. In a B plus Tree, all the data is at the bottom and all nodes over that index nodes in multiple levels. B plus Trees are implemented in many relational database products. Insertion, deletion, and update operations, of course, are provided together with those implementations to guarantee that the B plus Tree continues to be balanced. B stands for balanced. So, if the tree over time would deteriorate, so would the search time. So, it's very, very important that the distance from the route to the base level is the same at all times. It is actually quite rare that an overflow at a lower level will propagate far up the index. Maybe once, maybe twice is what is most common, and that in and by itself is rare.

<h1>23 - Static Hashing Part I</h1> Let me finally show you a completely different approach to primary data organization. It's called static hashing. The idea in static hashing is the following: You have a very large space of key values. Think for example the email addresses of regular users. So, that's your key value space. You have a hash function h. The job of the hash function is to take keys from the key value space and match them into an atlas in a pocket directory. The addresses it can map to would be at where zero, one, two, etc up to a minus one. A good hash function will have the following characteristics. First, it would need to distribute the values that it hits uniformly or with the address space. So, given the keys that are actually used in the application, it would have to uniformly map to this space. Also, it has to fill the space as much as possible because this by itself will take more than just a few blocks and therefore it needs to be stored somewhere and therefore a piece of it might indeed be stored on disk and itself be subject to being ported between disk and memory. So, it's important that pockets are full as much as possible. Contrary to that, it's also important to avoid collisions, and I'll explain a little bit later why it's important. So, collisions occur when the hash function maps actual keys that occur to the same space too many times. Now, let's take a little bit more look at what happens then and how the hash table is actually built.

<h1>24 - Static Hashing Part 2</h1> Let me start out by telling you how the data actually get inserted. Here no data has been inserted yet, all we have is the bucket directory and we have space in the bucket directory to put block addresses in that would point to the places where data records are stored. The hash function will map that key to an address. In this case here, it mapped it to the address two. In address two, sits a block address that block address points to a data block. On that data block, we stick that first record which had that key and now we proceed like that. It might be that additional records will go into this right now or later on but as we look at how this evolves you're going to get data blocks gradually allocated. Remember that several records can be stored on each block. There might not be additional blocks added for every single record we insert. Also remember, that as we insert the data an important job of the hash function is to uniformly allocate that data here. With the example that we have used we got room for about 20 records on each block. What happened here is that we have inserted data here. These are all partially full but now we actually had a second time that this address was hid, this data block here was full and therefore, a second data block is allocated to continue to store records that were mapped in by the hash function to this bucket directory address. I get more filled in here. Here's a second one that requires two levels more and more. It proceeds like this. Now the hash structure has been built, the data has been inserted and now we want to use this hash structure to find a record with a particular key value. We take the key value, we give it to the hash function and let's say the hash function returns this address in the bucket directory. In that is the block address we need to follow to get to the data block on which the record with that key value is stored if it's in the database. We need to access a block of the bucket directory. We need to access a block of data and then we'll look at whether the record is there or not. If the bucket directory page that we need is not in memory then it's going to cost one block access to get it up to memory. Then following the Pointer it's going to cost us one block access to get to the data, that's a grand total of two. Each one, one to two of them, each one cost 10 milliseconds. The cost is going to be about 10 to 20 milliseconds to look-up the data on it. It could be that a big portion of the bucket directory would actually fit in memory and stay there. If that's the case, then looking this one up would basically, we used a hash function, we get the address of that but that we don't pay for because it already in memory. We follow the pointer so we pay one to pull this block up and we find the record. That's a single one, that's only 10 milliseconds. What is interesting about this is, all the different look-up times we have seen they dependent on the size of the data, dependent on the size of the index et cetera. This one appears to be at least semi constant. There exist dynamic hash functions that will actually allow this adress space here to be expanded as to keep the access cost constant. Of course, if we cannot dynamically expand and we continue inserting data more and more and more here than this organization is going to deteriorate. The more data we insert with the same bucket directory size, the longer these lists of blocks we'll get. Notice that the way they were built was such that data is simply inserted in the order it arrives here when we run out of space. We go to the next one and the next one. If change like this become 20, 50, a 100 blocks long then the search in one of these linked lists is actually going to be like searching in a heap. It's very important to make sure that data is uniformly distributed. That this has the appropriate size to guarantee that this does not get too deep. In the previous data organization examples, I've illustrated them by using a phone book example. Forget about it here. They exist no such contraption.

